{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP53Gy85bgZeHTxqstMWM/y"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy # 코랩용 모듈 설치문"
      ],
      "metadata": {
        "id": "zXJb2Bb9xJmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQQ9X55hwQSo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from konlpy.tag import Okt\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive, files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "df = pd.read_csv('/content/drive/MyDrive/stocker/daum_news_samsung.csv')\n",
        "df_stock = pd.read_csv('/content/drive/MyDrive/stocker/samsung_stock.csv')\n",
        "df['date'] = pd.to_datetime(df['날짜']).dt.date\n",
        "\n",
        "# 텍스트 정제 함수\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^가-힣a-zA-Z0-9\\s]', '', str(text))\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "df['clean_title'] = df['뉴스제목'].apply(clean_text) # 뉴스제목 정제\n",
        "\n",
        "print(df.head(5))"
      ],
      "metadata": {
        "id": "cDEIyXVGwR3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "okt = Okt() # 우선 Okt 사용\n",
        "stopwords = {'은', '는', '이', '가', '하', '의', '에', '을', '를', '도', '과', '와', '으로', '로', '에서', '하다', '고'}\n",
        "\n",
        "def preprocess(text):\n",
        "    tokens = okt.morphs(text, stem=True)\n",
        "    meaningful_words = [word for word in tokens if word not in stopwords and len(word) > 1]\n",
        "    return ' '.join(meaningful_words)\n",
        "\n",
        "df['processed_title'] = df['clean_title'].apply(preprocess)\n",
        "\n",
        "print(df.head(5))"
      ],
      "metadata": {
        "id": "vL4fzZNKwVvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"snunlp/KR-FinBERT\" # Finbert 모델 지정\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name) # 토크나이징징\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "label_map = {0: 'negative', 1: 'neutral', 2: 'positive'}"
      ],
      "metadata": {
        "id": "6mdDywW4y0yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 감성 분석 함수\n",
        "def predict_sentiment(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128, padding=True)\n",
        "    outputs = model(**inputs)\n",
        "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1) # 확률값으로 변환\n",
        "    conf, pred_label = torch.max(probs, dim=1)\n",
        "    return label_map[pred_label.item()], conf.item()\n",
        "\n",
        "def get_sentiment_score(text):\n",
        "    if not text.strip():\n",
        "        return 0\n",
        "    label, conf = predict_sentiment(text)\n",
        "\n",
        "    # 긍부정 확률값 반환\n",
        "    if label == 'positive':\n",
        "        return 1 * conf\n",
        "    elif label == 'negative':\n",
        "        return -1 * conf\n",
        "    else:\n",
        "        return 0"
      ],
      "metadata": {
        "id": "cr_MeWwUwXNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['sentiment_score'] = df['processed_title'].progress_apply(get_sentiment_score) # 날짜 기준으로 정렬\n",
        "\n",
        "daily_sentiment = df.groupby('날짜').agg(\n",
        "    avg_sentiment=('sentiment_score', 'mean'),\n",
        ").reset_index()\n",
        "\n",
        "print(daily_sentiment.head())"
      ],
      "metadata": {
        "id": "uBXio2KjweLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_stock.isnull().sum())\n",
        "df_stock.dropna(inplace=True) # 결측치 있는 행 삭제\n",
        "\n",
        "df_stock = df_stock[df_stock['거래량'] > 0] # 거래량 0인 행 삭제\n",
        "\n",
        "df_stock = df_stock.sort_values('Date')\n",
        "df_stock['Close_Change_Rate'] = (df_stock['종가'].pct_change() * 100) # 변동률\n",
        "\n",
        "print(df_stock[['날짜', '종가', '거래량', '변동률(%)', '상승 여부']].head(10))"
      ],
      "metadata": {
        "id": "nqKUOOaewgyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_stock = df_stock.loc[:, ~df_stock.columns.duplicated()]\n",
        "\n",
        "# 날짜 컬럼 이름 통일일\n",
        "daily_sentiment.rename(columns={'Date': '날짜'}, inplace=True)\n",
        "df_stock.rename(columns={'Date': '날짜'}, inplace=True)\n",
        "\n",
        "# 날짜 형식 변호나 & 시간 정규화\n",
        "daily_sentiment['날짜'] = pd.to_datetime(daily_sentiment['날짜']).dt.normalize()\n",
        "df_stock['날짜'] = pd.to_datetime(df_stock['날짜']).dt.normalize()\n",
        "\n",
        "# 날짜 기준으로 두 데이터 내부 조인\n",
        "df_merged = pd.merge(\n",
        "    df_stock,\n",
        "    on='날짜',\n",
        "    how='inner',\n",
        "    suffixes=('_stock', '_sentiment')\n",
        ")\n",
        "\n",
        "# 날짜 컬럼 정리 (날짜 겹침)\n",
        "if '날짜_sentiment' in df_merged.columns:\n",
        "    df_merged.drop(columns=['날짜_sentiment'], inplace=True)\n",
        "\n",
        "if '날짜_stock' in df_merged.columns:\n",
        "    df_merged.rename(columns={'날짜_stock': '날짜'}, inplace=True)\n",
        "\n",
        "if 'avg_sentiment' in df_merged.columns: # 컬럼명 변경\n",
        "    df_merged.rename(columns={'avg_sentiment': '감성 점수'}, inplace=True)\n",
        "\n",
        "df_merged['감성 점수'] = df_merged['감성 점수'].fillna(0) # 결측값 0으로 채우기\n",
        "\n",
        "df_merged = df_merged.sort_values('날짜')\n",
        "df_merged['변동률(%)'] = df_merged['종가'].pct_change() * 100 # 변동률 재계산\n",
        "df_merged['변동률(%)'] = df_merged['변동률(%)'].fillna(0)\n"
      ],
      "metadata": {
        "id": "LVCXfi82woBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_merged.head())\n",
        "print(df_merged.columns)\n",
        "\n",
        "df_merged.to_csv('data_preprocessing.csv', index=False)\n",
        "files.download('data_preprocessing.csv')"
      ],
      "metadata": {
        "id": "52UV64KawuTE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}